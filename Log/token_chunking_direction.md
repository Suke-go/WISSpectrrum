# Token-Based Chunking 改善メモ

## 現状の課題
- `summary/summarize_pdf.py:134` の `chunk_text` は単純に文字数で分割しているため、句読点やマルチバイト文字列の配置によってトークン長が大きく変動する。
- トークン数が上振れしたチャンクが `call_openai` に渡ると、`max_output_tokens` を超えて再試行や途中切り捨てが発生し、全体の処理時間が悪化する。

## 修正の方向性
1. **tiktoken によるトークン制御**
   - `tiktoken` を利用してモデルごとのエンコーダを取得し、トークン数ベースでチャンク幅を管理する。
   - 目標トークン数 (`target_token_length`) としきい値 (`soft_limit`, `hard_limit`) を設定し、超過分は境界まで巻き戻して切り分ける。
2. **文境界での調整**
   - トークン数で区切った後、句点 (`。`, `.`) や改行を優先して境界調整を行い、文途中で分割しないようにする。
   - 余裕がある場合は `regex` ベースでセンテンスを検出し、トークン上限を超えない範囲でまとめ直す。
3. **フォールバック処理**
   - どうしても長い単一文で上限を超える場合は、既存の文字数ベース分割にフォールバックして処理を継続する。
4. **パラメータ化**
   - CLI オプションまたは設定でトークン幅を調整可能にし、モデル変更時も柔軟に対応できるようにする。
5. **監視と検証**
   - 分割後のトークン長分布をログ出力し、再試行率やレスポンス切り捨て率の変化を継続的にモニタリングする。

## 次のステップ
- `requirements.txt` に `tiktoken` を追加し、エンコーダ初期化ヘルパーを実装する。
- `chunk_text` をトークンベースロジックへ置き換え、単体テキストでのベンチマークを実施する。
- LLM 応答の再試行回数・処理時間を記録して改善効果を評価する。

